#!/usr/bin/env bash
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Install Apache HBase.
#

set -x
set -e

################################################################################
# Initialize variables
################################################################################

CLOUD_PROVIDER=
while getopts "c:" OPTION; do
  case $OPTION in
  c)
    CLOUD_PROVIDER="$OPTARG"
    ;;
  esac
done

HBASE_VERSION=${HBASE_VERSION:-0.89.20100924}
HBASE_HOME=/usr/local/hbase-$HBASE_VERSION

function update_repo() {
  if which dpkg &> /dev/null; then
    sudo apt-get update
  elif which rpm &> /dev/null; then
    yum update -y yum
  fi
}

# Install a list of packages on debian or redhat as appropriate
function install_packages() {
  if which dpkg &> /dev/null; then
    apt-get update
    apt-get -y install $@
  elif which rpm &> /dev/null; then
    yum install -y $@
  else
    echo "No package manager found."
  fi
}

function install_hbase() {
  if ! id hadoop &> /dev/null; then
    useradd hadoop
  fi

  # up file-max
  sysctl -w fs.file-max=65535
  # up ulimits
  echo "root soft nofile 65535" >> /etc/security/limits.conf
  echo "root hard nofile 65535" >> /etc/security/limits.conf
  ulimit -n 65535
  # up epoll limits; ok if this fails, only valid for kernels 2.6.27+
  set +e
  sysctl -w fs.epoll.max_user_instances=65535 > /dev/null 2>&1
  set -e
  # if there is no hosts file then provide a minimal one
  [ ! -f /etc/hosts ] &&  echo "127.0.0.1 localhost" > /etc/hosts

  # Reformat sdb as xfs
  #umount /mnt
  #mkfs.xfs -f /dev/sdb
  #mount -o noatime /dev/sdb /mnt
  # Probe for additional instance volumes
  # /dev/sdb as /mnt is always set up by base image
  #DFS_NAME_DIR="/mnt/hadoop/dfs/name"
  #DFS_DATA_DIR="/mnt/hadoop/dfs/data"
  #i=2
  #for d in c d e f g h i j k l m n o p q r s t u v w x y z; do
  # m="/mnt${i}"
  # mkdir -p $m
  # mkfs.xfs -f /dev/sd${d}
  # if [ $? -eq 0 ] ; then
  #  mount -o noatime /dev/sd${d} $m > /dev/null 2>&1
  #  if [ $i -lt 3 ] ; then # no more than two namedirs
  #   DFS_NAME_DIR="${DFS_NAME_DIR},${m}/hadoop/dfs/name"
  #  fi
  #  DFS_DATA_DIR="${DFS_DATA_DIR},${m}/hadoop/dfs/data"
  #  i=$(( i + 1 ))
  # fi
  #done

  # install HBase tarball
  hbase_tar_url=http://archive.apache.org/dist/hbase/hbase-$HBASE_VERSION/hbase-${HBASE_VERSION}-bin.tar.gz
  hbase_tar_file=`basename $hbase_tar_url`
  hbase_tar_md5_file=`basename $hbase_tar_url.md5`

  curl="curl --retry 3 --silent --show-error --fail"
  for i in `seq 1 3`;
  do
    $curl -O $hbase_tar_url
    $curl -O $hbase_tar_url.md5
    if md5sum -c $hbase_tar_md5_file; then
      break;
    else
      rm -f $hbase_tar_file $hbase_tar_md5_file
    fi
  done

  if [ ! -e $hbase_tar_file ]; then
    echo "Failed to download $hbase_tar_url. Aborting."
    exit 1
  fi

  tar zxf $hbase_tar_file -C /usr/local
  rm -f $hbase_tar_file $hbase_tar_md5_file

  echo "export HBASE_HOME=$HBASE_HOME" >> ~root/.bashrc
  echo 'export PATH=$JAVA_HOME/bin:$HBASE_HOME/bin:$PATH' >> ~root/.bashrc
}

update_repo
install_hbase
